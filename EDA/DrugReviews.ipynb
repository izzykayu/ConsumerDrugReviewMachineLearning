{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-26\n",
      "['uid', 'urlDrugName', 'rating', 'effectiveness', 'sideEffects', 'condition', 'benefitsReview', 'sideEffectsReview', 'commentsReview']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urlDrugName</th>\n",
       "      <th>rating</th>\n",
       "      <th>effectiveness</th>\n",
       "      <th>sideEffects</th>\n",
       "      <th>condition</th>\n",
       "      <th>benefitsReview</th>\n",
       "      <th>sideEffectsReview</th>\n",
       "      <th>commentsReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>botox</td>\n",
       "      <td>10</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>No Side Effects</td>\n",
       "      <td>aging</td>\n",
       "      <td>the treatment (eyes &amp; forehead) was highly effective and lasted 6 months each of the 3 times I was treated. I have a very deep vertical line between my eyes which was all but invisible with each t...</td>\n",
       "      <td>none</td>\n",
       "      <td>Botox was administered to my forehead and outer eye area (crows feet) via small gauge needle. Totally painless. It took about 3 days for the drug to take effect but once it did, it kept the wrinkl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>triphasil</td>\n",
       "      <td>10</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>No Side Effects</td>\n",
       "      <td>contraception</td>\n",
       "      <td>general medical benefits:  contraception, hormone management.  Personal benefits: i controlled my weight better, skin looked more radiant and menstruation pains were alot less than when i was not ...</td>\n",
       "      <td>side effects - when leaving the pill - i started losing my hair - alot of it</td>\n",
       "      <td>daily dosage, usually taken at night time, managed my weight, and helped with my skin as well as a contraceptive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     urlDrugName  rating     effectiveness      sideEffects      condition  \\\n",
       "808        botox      10  Highly Effective  No Side Effects          aging   \n",
       "2856   triphasil      10  Highly Effective  No Side Effects  contraception   \n",
       "\n",
       "                                                                                                                                                                                               benefitsReview  \\\n",
       "808   the treatment (eyes & forehead) was highly effective and lasted 6 months each of the 3 times I was treated. I have a very deep vertical line between my eyes which was all but invisible with each t...   \n",
       "2856  general medical benefits:  contraception, hormone management.  Personal benefits: i controlled my weight better, skin looked more radiant and menstruation pains were alot less than when i was not ...   \n",
       "\n",
       "                                                                 sideEffectsReview  \\\n",
       "808                                                                           none   \n",
       "2856  side effects - when leaving the pill - i started losing my hair - alot of it   \n",
       "\n",
       "                                                                                                                                                                                               commentsReview  \n",
       "808   Botox was administered to my forehead and outer eye area (crows feet) via small gauge needle. Totally painless. It took about 3 days for the drug to take effect but once it did, it kept the wrinkl...  \n",
       "2856                                                                                         daily dosage, usually taken at night time, managed my weight, and helped with my skin as well as a contraceptive  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import datetime\n",
    "todaysdate = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(todaysdate)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "g_map = {'F': 1, 'M': 2, 'OTHER': 3, '': 0}\n",
    "\n",
    "def to_string(s):\n",
    "    \"\"\"\n",
    "    makes to string and changes encoding\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(s)\n",
    "    except:\n",
    "        #Change the encoding type if needed\n",
    "        return s.encode('utf-8')\n",
    "    \n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "pos_dic = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def pos_check(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_dic[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "# df['noun_count'] = df['NOTE_TEXT'].apply(lambda x: pos_check(x, 'noun'))\n",
    "# df['verb_count'] = df['NOTE_TEXT'].apply(lambda x: pos_check(x, 'verb'))\n",
    "# df['adj_count'] = df['NOTE_TEXT'].apply(lambda x: pos_check(x, 'adj'))\n",
    "# df['adv_count'] = df['NOTE_TEXT'].apply(lambda x: pos_check(x, 'adv'))\n",
    "# df['pron_count'] = df['NOTE_TEXT'].apply(lambda x: pos_check(x, 'pron'))\n",
    "# print(df[['noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']].head(10))\n",
    "\n",
    "\n",
    "se_map = {'No Side Effects': 0,\n",
    "          'Mild Side Effects': 1,\n",
    "          'Moderate Side Effects':2,\n",
    "          'Severe Side Effects' :3,\n",
    "          'Extremely Severe Side Effects': 4}\n",
    "\n",
    "effect_map = {'Ineffective': 0,\n",
    "              'Marginally Effective':1,\n",
    "              'Moderately Effective': 2,\n",
    "          'Considerably Effective' :3,\n",
    "          'Highly Effective': 4}\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "train = pd.read_csv(\"../data/drugLibTrain_raw.tsv\", delimiter='\\t')\n",
    "train = train.rename({'Unnamed: 0': 'uid'}, axis=1)\n",
    "test = pd.read_csv(\"../data/drugLibTest_raw.tsv\", delimiter='\\t')\n",
    "test = test.rename({'Unnamed: 0': 'uid'}, axis=1)\n",
    "review_text_columns = ['benefitsReview', 'sideEffectsReview', 'commentsReview']\n",
    "\n",
    "def replace_missing_texts(train, review_text_columns):\n",
    "    for i in review_text_columns:\n",
    "        train[i] = train[i].fillna(\"fillna\")\n",
    "        test[i] = test[i].fillna(\"fillna\")\n",
    "    \n",
    "print(list(train.columns))\n",
    "\n",
    "train[[ 'urlDrugName', 'rating', 'effectiveness', 'sideEffects',\n",
    "       'condition', 'benefitsReview', 'sideEffectsReview', 'commentsReview']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pprint\n",
    "articles = list(train['commentsReview'][0:20])#'benefitsReview'])\n",
    "import unicodedata\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer()\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    tknzr = TweetTokenizer()\n",
    "    s = \" \".join(tknzr.tokenize(s))\n",
    "    s = re.sub(r\"\\d\", \"d\", s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?]+)\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?-]+\", r\" \", s)\n",
    "    s = to_string(s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def tokenize_multiple_return_strings(articles):\n",
    "    art_new = [normalizeString(ar) for ar in articles]\n",
    "    tokens = flatten([word_tokenize(ar) for ar in articles])\n",
    "    \n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "    bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "    print(bow_simple.most_common(10))\n",
    "    return art_new\n",
    "    \n",
    "# tokenize_multiple_return_strings(articles[2:5])\n",
    "#[re.findall(pattern='depress', string=x) for x in set(pd.DataFrame(train.condition.value_counts()).index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger!\n",
      "['LemmatiZbenefitsReview', 'LemmatiZsideEffectsReview', 'LemmatiZcommentsReview']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text2 = normalizeString(text)\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in\n",
    "              nltk.word_tokenize(sent)]\n",
    "\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    except:\n",
    "        tokens = tokens\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "    Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    Verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "\n",
    "    pre_proc_text = \" \".join([prat_lemmatize(token, tag) for token, tag in tagged_corpus])\n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "\n",
    "print(\"tagger!\")\n",
    "processed_reviews_columns = [\"LemmatiZ\" + i for i in review_text_columns]\n",
    "print(processed_reviews_columns)\n",
    "\n",
    "\n",
    "def preproc_reviews(train, test, colname):\n",
    "    train_text = train[colname].to_list()\n",
    "    test_text = test[colname].to_list()\n",
    "#\n",
    "    x_train_preprocessed = []\n",
    "    for i in train_text:\n",
    "        x_train_preprocessed.append(preprocessing(i))\n",
    "\n",
    "    x_test_preprocessed = []\n",
    "    for i in test_text:\n",
    "        x_test_preprocessed.append(preprocessing(i))\n",
    "        \n",
    "    return x_train_preprocessed, x_test_preprocessed\n",
    "\n",
    "\n",
    "\n",
    "#[preprocessing(article) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_text' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-76aee1427791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_train_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_text' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "len(train_text)\n",
    "\n",
    "#\n",
    "x_train_preprocessed = []\n",
    "for i in train_text:\n",
    "    x_train_preprocessed.append(preprocessing(i))\n",
    "\n",
    "x_test_preprocessed = []\n",
    "for i in test_text:\n",
    "    x_test_preprocessed.append(preprocessing(i))\n",
    "\n",
    "print(\"INTRO TO GENSIM\")\n",
    "# what is a word vector/embeddings\n",
    "# trained from larger corpus\n",
    "# multidimensional array\n",
    "# with these vectors, caN see relationships between the words\n",
    "# e.g., span is to madrid as italy is to rome\n",
    "# based on how words how near the words are in the text\n",
    "# LDA statistical analysis of texts\n",
    "# gensim allows you to build corpora\n",
    "# documents list of strings\n",
    "\n",
    "my_documents = list(train['condition'])\n",
    "\n",
    "tokenize_multiple_return_strings(my_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They\n",
    "# give us insight into relationships between words in a corpus.\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "print(\"********tokenized docs******\")\n",
    "print(tokenized_docs)\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "print(\"*****Gensim Dictionary*****\")\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "print(\"****Gensim Corpus****\")\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from gensim.corpora.dictionary import Dictionary\n",
    "articles = tokenized_docs\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "doc = corpus[4]\n",
    "\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_step_rating_vars = ['effectiveness', 'sideEffects']\n",
    "free_form_text_vars = ['benefitsReview', 'sideEffectsReview', 'commentsReview']\n",
    "from afinn import Afinn\n",
    "example_text = \"muscle pain, loss of mobility, depresion, headaches i was admitted to hospital with chest pains, thought to be heart problems, it was caused by the tightening of my chest muscles\"\n",
    "afinnSentiScorer = Afinn(emoticons=True)\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "print(example_text)\n",
    "print('**************\\nAfinn Sentiment Score:\\n', afinnSentiScorer.score(example_text))\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "print('Vader Sentiment Intensity Analyzer scores:\\n', analyzer.polarity_scores(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nEFFECTIVENESS\\n===========================\\n')\n",
    "print(train['effectiveness'].value_counts())\n",
    "print('===========================\\n')\n",
    "print(train['effectiveness'].describe())\n",
    "print('\\n===========================\\n\\nSIDE EFFECTS\\n===========================\\n')\n",
    "print(train['sideEffects'].value_counts())\n",
    "print('===========================\\n')\n",
    "print(train['sideEffects'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "moonrise =  [\"#F3DF6C\", \"#CEAB07\", \"#D5D5D3\", \"#24281A\",\"#798E87\",\n",
    "             \"#C27D38\", \"#CCC591\", \"#29211F\",\"#85D4E3\", \"#F4B5BD\",\n",
    "             \"#9C964A\", \"#CDC08C\", \"#FAD77B\"]\n",
    "\n",
    "grandBudapest = [\"#E6A0C4\", \"#C6CDF7\", \"#5B1A18\", \"#D67236\",\n",
    "                  \"#D8A499\", \"#7294D4\",\"#F1BB7B\", \"#FD6467\"]\n",
    "\n",
    "sns.set(style=\"ticks\", palette=grandBudapest)#\"ch:.25\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"sideEffects\", y=\"rating\",\n",
    "            data=train)\n",
    "\n",
    "sns.despine(offset=5, trim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# \"Melt\" the dataset to \"long-form\" or \"tidy\" representation\n",
    "tidy_data = pd.melt(train, \"sideEffects\", var_name=\"rating\")\n",
    "\n",
    "# Initialize the figure\n",
    "f, ax = plt.subplots()\n",
    "sns.despine(bottom=True, left=True)\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(x=\"value\", y=\"rating\", hue=\"sideEffects\",\n",
    "              data=tidy_data, dodge=True, jitter=True,\n",
    "              alpha=.25, zorder=1)\n",
    "\n",
    "# Show the conditional means\n",
    "sns.pointplot(x=\"value\", y=\"rating\", hue=\"sideEffects\",\n",
    "              data=tidy_data, dodge=.532, join=False, palette=\"dark\",\n",
    "              markers=\"d\", scale=.75, ci=None)\n",
    "\n",
    "# Improve the legend \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[3:], labels[3:], title=\"\",\n",
    "          handletextpad=0, columnspacing=1,\n",
    "          loc=\"lower right\", ncol=3, frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "list(calendar.day_abbr)\n",
    "# outfile = open(pn + str(todaysdate) + '_N2C2_test_Jun1_2018.txt', 'a')\n",
    "# counter = 0\n",
    "# df = pd.read_csv(pn +'N2C2_test_Jun1_2018.csv')\n",
    "# print(df.info())\n",
    "# for index, row in df.iterrows():\n",
    "#     counter += 1\n",
    "#     new_note = clean_mimic(row['text'])\n",
    "#     print(counter)\n",
    "#     outfile.write(new_note + os.linesep)\n",
    "# outfile.close()\n",
    "# # #     print(new_note)\n",
    "# print(\"total docs written out\", str(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reindex(list(calendar.day_abbr), level='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P.J.', 'has indicated that she is injecting insulin to treat her diabetes .', 'What questions might be asked to evaluate P.J.’s use of and response to insulin ?', 'M.B., a 35-year-old man, presents to the ED with a chief complaint of chest palpitations for 4 hours.', 'He relates a history of many similar self-terminating episodes since he was a teenager.', 'He took an unknown medication 5 years ago that decreased the occurrence of the pal- pitations, but he stopped taking it because of side effects.', 'M.B.’s vital signs are BP, 96/68 mm Hg; pulse, 226 beats/minute, irregular; respiratory rate, 15 breaths/ minute; and temperature, 98.7◦F.', 'A rhythm strip confirms AF, with a QRS width varying from 0.08 to 0.14 seconds.', 'To control the ventricular rate, 10 mg IV verapamil is admin- istered for 2 minutes.', 'Within 2 minutes of completing the infusion, VF is noted on the monitor.', 'M.B.', 'is defibrillated, and normal sinus rhythm is restored.', 'A subsequent ECG demonstrates a P-R interval of 100 ms (normal, 120 to 200 ms) and delta waves, compatible with WPW.', 'What is WPW syndrome?']\n",
      "His admission electrocardiogram demonstrated a sinus rhythm,[NEWLINE] nonspecific inferior/lateral T wave changes, low QRS voltages[NEWLINE] in the limb leads, and T wave changes in V5 and V6 when[NEWLINE] compared with an electrocardiogram dated dddd-dd-dd\n",
      "['In', 'the', 'Emergency', 'Department', ',', 'he', 'was', 'found', 'to', 'have', 'systolic', 'blood', 'pressure', 'of', '85', '.', 'He', 'was', 'given', '6', 'liters', 'of', 'intravenous', 'fluids', 'and', 'transiently', 'started', 'on', 'dopamine', 'for', 'a', 'systolic', 'blood', 'pressure', 'in', 'the', '80.s', 'PAST', 'MEDICAL', 'HISTORY', ':', '1', '.', 'Coronary', 'artery', 'disease', 'with', 'diffuse', '3-vessel', 'disease', ';', 'right-dominant', ',', 'status', 'post', 'proximal', 'left', 'circumflex', 'stent', 'in', 'dddd-dd-dd', 'with', 'occlusion', 'of', 'the', 'distal', 'left', 'circumflex', ';', 'status', 'post', 'right', 'coronary', 'artery', 'stent', 'on', 'dddd-dd-dd', '(', 'no', 'percutaneous', 'coronary', 'intervention', 'to', '99', '%', 'diagonal', 'left', 'circumflex', ',', '80', '%', 'small', 'proximal', 'left', 'anterior', 'descending', 'artery', ',', 'or', '80', '%', 'small', 'distal', 'left', 'anterior', 'descending', 'artery', ')', '2', '.', 'Congestive', 'heart', 'failure', '(', 'with', 'an', 'ejection', 'fraction', 'of', '15', '%', 'to', '20', '%', ')', '3', '.', 'Type', '2', 'diabetes', 'with', 'neuropathy', '.', '4', '.', 'Hypertension', '.', '5', '.', 'Diverticulosis']\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "bad character range ?-; at position 10",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7b546d3313d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample_text_pharmacytextbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_example_text_mimic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0mtokenize_multiple_return_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-7b546d3313d8>\u001b[0m in \u001b[0;36mtokenize_multiple_return_strings\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeroStartTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NOTE_TEXT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0marticles_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreProc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7b546d3313d8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeroStartTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NOTE_TEXT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0marticles_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreProc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7b546d3313d8>\u001b[0m in \u001b[0;36mpreProc\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Prednisone 40 mg once per day times three days; then 20mg once per day x 11 days.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     tokens = [word for sent in sent_tokenize(text2) for word in\n",
      "\u001b[0;32m<ipython-input-33-7b546d3313d8>\u001b[0m in \u001b[0;36mnormalizeString\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodeToAscii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m#s = re.sub(r\"([^a-zA-Z.!?-].\\d+)\", r\"\\1\", s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-zA-Z.!?-;\\:]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0;32m--> 416\u001b[0;31m                            not nested and not items))\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bad character range %s-%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                     \u001b[0msetappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRANGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: bad character range ?-; at position 10"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "## izzy metzger Feb 26th. pre-processing texts \n",
    "## this file pre-processes text either in a csv, json, or bz2 \n",
    "# Usage: python3 preprocessing.py <inFile> <csv|bz2|txt> <outFile> <csv|bz2|txt>\n",
    "from __future__ import print_function\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "import datetime\n",
    "import ujson\n",
    "import bz2\n",
    "import os\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "example_text_pharmacytextbook = \"\"\"P.J. has indicated that she is injecting insulin to treat her diabetes . What questions might be asked to evaluate P.J.’s use of and response to insulin ?\n",
    "\n",
    "M.B., a 35-year-old man, presents to the ED with a chief complaint of chest palpitations for 4 hours. He relates a history of many similar self-terminating episodes since he was a teenager. He took an unknown medication 5 years ago that decreased the occurrence of the pal- pitations, but he stopped taking it because of side effects. M.B.’s vital signs are BP, 96/68 mm Hg; pulse, 226 beats/minute, irregular; respiratory rate, 15 breaths/ minute; and temperature, 98.7◦F. A rhythm strip confirms AF, with a QRS width varying from 0.08 to 0.14 seconds. To control the ventricular rate, 10 mg IV verapamil is admin- istered for 2 minutes. Within 2 minutes of completing the infusion, VF is noted on the monitor. M.B. is defibrillated, and normal sinus rhythm is restored. A subsequent ECG demonstrates a P-R interval of 100 ms (normal, 120 to 200 ms) and delta waves, compatible with WPW. What is WPW syndrome? \"\"\"\n",
    "\n",
    "second_example_text_mimic = \"\"\"\n",
    "In the Emergency Department, he was found to have systolic blood pressure of 85.  He was given 6 liters of intravenous fluids and transiently started on dopamine for a systolic blood pressure in the 80.s\n",
    "\n",
    "PAST MEDICAL HISTORY: \n",
    "1.  Coronary artery disease with diffuse 3-vessel disease; right-dominant, status post proximal left circumflex stent in [**2682-5-27**] with occlusion of the distal left circumflex;\n",
    "status post right coronary artery stent on [**2682-7-14**] (no percutaneous coronary intervention to 99% diagonal left circumflex, 80% small proximal left anterior descending\n",
    "artery, or 80% small distal left anterior descending artery)\n",
    "2.  Congestive heart failure (with an ejection fraction of 15% to 20%)\n",
    "3.  Type 2 diabetes with neuropathy.\n",
    "4.  Hypertension.\n",
    "5.  Diverticulosis\n",
    "\"\"\"\n",
    "\n",
    "def mkdir_if_not_exist(path):\n",
    "    \"\"\"\n",
    "    function to make directory\n",
    "    path is a string (e.g., data/text_outputs)\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "# plain ASCII, THNX stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "print(sent_tokenize(example_text_pharmacytextbook))\n",
    "def remove_brackets(s):\n",
    "    \"\"\"\n",
    "    since mimic has lots of brackets\n",
    "    input:string s\n",
    "    output s without brackets but yyyy-mm-dd instead except using \"d\" for digit\n",
    "    \"\"\"\n",
    "    s = re.sub('\\[\\*\\*.*\\*\\*\\]', 'dddd-dd-dd', s).replace('  ', ' ')\n",
    "    return s\n",
    "\n",
    "example_mimiciii = \"His admission electrocardiogram demonstrated a sinus rhythm,[NEWLINE] nonspecific inferior/lateral T wave changes, low QRS voltages[NEWLINE] in the limb leads, and T wave changes in V5 and V6 when[NEWLINE] compared with an electrocardiogram dated [**2682-8-30**]\"\n",
    "\n",
    "print(remove_brackets(example_mimiciii))\n",
    "\n",
    "print([word for sent in nltk.sent_tokenize(remove_brackets(second_example_text_mimic)) for word in\n",
    "              word_tokenize(sent)])\n",
    "def normalizeString(s):\n",
    "    #tknzr = TweetTokenizer()\n",
    "    #s = \" \".join(tknzr.tokenize(s))\n",
    "    #s = re.sub(r\"\\d\", \"d\", s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #s = re.sub(r\"([^a-zA-Z.!?-].\\d+)\", r\"\\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?-;]+\", r\" \", s)\n",
    "    s = to_string(s).strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "def preProc(text):\n",
    "    \"\"\"\n",
    "    text is a single string, for example: \n",
    "    text = \"Prednisone 40 mg once per day times three days; then 20mg once per day x 11 days.\"\n",
    "    \"\"\"\n",
    "    text2 = normalizeString(text)\n",
    "\n",
    "    tokens = [word for sent in sent_tokenize(text2) for word in\n",
    "              word_tokenize(sent)]\n",
    "\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    except:\n",
    "        tokens = tokens\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "    Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    Verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def pratLemmatiz(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "\n",
    "    pre_proc_text = \" \".join([pratLemmatiz(token, tag) for token, tag in tagged_corpus])\n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "def tokenize_multiple_return_strings(articles):\n",
    "    \"\"\"\n",
    "    articles: a list of strings e.g.,\n",
    "    articles = zeroStartTrain['NOTE_TEXT'].to_list()\n",
    "    \"\"\"\n",
    "    articles_new = [preProc(ar) for ar in articles]\n",
    "    pprint.pprint(articles_new)\n",
    "    tokens = flatten([word_tokenize(ar) for ar in articles_new])\n",
    "    \n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "    lower_tokens = [t.lower() for t in tokens if t not in list(string.punctuation)]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "    bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "    print(bow_simple.most_common(10))\n",
    "    #print(list(string.punctuation))\n",
    "    return articles_new\n",
    "\n",
    "articles = [example_text_pharmacytextbook, second_example_text_mimic]\n",
    "tokenize_multiple_return_strings(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProc(text):\n",
    "    \"\"\"\n",
    "    text is a single string, for example: \n",
    "    text = \"Prednisone 40 mg once per day times three days; then 20mg once per day x 11 days.\"\n",
    "    \"\"\"\n",
    "    text2 = normalizeString(text)\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in\n",
    "              word_tokenize(sent)]\n",
    "\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    except:\n",
    "        tokens = tokens\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "    Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    Verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def pratLemmatiz(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "\n",
    "    pre_proc_text = \" \".join([pratLemmatiz(token, tag) for token, tag in tagged_corpus])\n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "\n",
    "\n",
    "# Command line options\n",
    "optargs = [\n",
    "    ('--inFile', {\n",
    "        'type': str,\n",
    "        'default': 'data/training_example.csv',\n",
    "        'help': 'Path to dataset with csv of text to pre-process',\n",
    "    }),\n",
    "    ('--inFileType', {\n",
    "        'type': str,\n",
    "        'default': 'csv',\n",
    "        'help': 'arguments include csv, json, bz2, or plaintext readers',\n",
    "    }),\n",
    "\t    ('--outFileDir', {\n",
    "        'type': str,\n",
    "        'default': 'data/pre_proc_texts/',\n",
    "        'help': 'path to directory for writing out pre-proc texts',\n",
    "    }),\n",
    "    ('--outFileType', {\n",
    "        'type': str,\n",
    "        'default': 'txt_doc',\n",
    "        'help': 'Path to writing out clean notes (for example, for embeddings)',\n",
    "    }),\n",
    "   \n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "for opt, config in optargs:\n",
    "    parser.add_argument(opt, **config)\n",
    "\n",
    "args = parser.parse_args()\n",
    "mkdir_if_not_exist(args.outFileDir)\n",
    "todaysdate = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(todaysdate)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''Convert data and normalize it in different ways.\n",
    "        For example: if you want to convert a text file to a fully cleaned file, then you run as following:\n",
    "            python3 preprocessing_text.py MIMIC2TEXT text <input_file> <output_file>\n",
    "        Input data format is just path to text file.\n",
    "    '''\n",
    "    if sys.argv[1].upper() == \"MIMIC2TEXT\":\n",
    "        MIMIC2TEXT(sys.argv[2],sys.argv[3])\n",
    "    elif sys.argv[1].upper() == \"ZEROSTART2TEXT\":\n",
    "        ZEROSTART2TEXT(sys.argv[2],sys.argv[3])\n",
    "    elif sys.argv[1].upper() == \"ZEROSTART2CSV\":\n",
    "        ZEROSTART2CSV(sys.argv[2],sys.argv[3])\n",
    "    elif sys.argv[1].upper() == \"TEXT2TEXT\":\n",
    "        TEXT2TEXT(sys.argv[2], sys.argv[3])\n",
    "    else:\n",
    "        print(\"Argument error: sys.argv[1] should belongs to \\\"MIMIC2TEXT/ZEROSTART2TEXT/ZEROSTART2CSV/TEXT2TEXT\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import open\n",
    "\n",
    "outfile = open(args.outFileDir + str(todaysdate) + '_N2C2_test_Jun1_2018.txt', 'a')\n",
    "counter = 0\n",
    "df = pd.read_csv(pn +'N2C2_test_Jun1_2018.csv')\n",
    "print(df.info())\n",
    "for index, row in df.iterrows():\n",
    "    counter += 1\n",
    "    new_note = clean_mimic(row['text'])\n",
    "    print(counter)\n",
    "    outfile.write"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

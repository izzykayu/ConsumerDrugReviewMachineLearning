{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize, word_tokenize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f = \"~/PycharmProjects/ConsumerDrugReviewMachineLearning/data/drugLib/drugLibTrain_raw.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(s):\n",
    "    try:\n",
    "        return str(s)\n",
    "    except:\n",
    "        #Change the encoding type if needed\n",
    "        return s.encode('utf-8')\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    tknzr = TweetTokenizer()\n",
    "    s = \" \".join(tknzr.tokenize(s))\n",
    "    s = re.sub(r\"\\d\", \"d\", s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?]+)\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?-]+\", r\" \", s)\n",
    "    s = to_string(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def preProc(text):\n",
    "    \"\"\"\n",
    "    text is a string, for example: \"Please keep humira refrigeraterd.\n",
    "    \"\"\"\n",
    "\n",
    "    text2 = normalizeString(text)\n",
    "\n",
    "    tokens = [word for sent in sent_tokenize(text2) for word in\n",
    "          word_tokenize(sent)]\n",
    "\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    except:\n",
    "        tokens = tokens\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "    Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    Verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def pratLemmatiz(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "\n",
    "\n",
    "    pre_proc_text = \" \".join([pratLemmatiz(token, tag) for token, tag in tagged_corpus])\n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "\n",
    "def bigram_preprocess(tokens, deacc=True, lowercase=True, errors='ignore',\n",
    "    stemmer=None, stopwords=None):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of tokens.\n",
    "    Split text into sentences and sentences into bigrams.\n",
    "    the bigrams returned are the tokens\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "\n",
    "    if len(tokens) >1:\n",
    "        for i in range(0,len(tokens)-1):\n",
    "            yield tokens[i] + '_' + tokens[i+1]\n",
    "\n",
    "    \n",
    "\n",
    "df = pd.read_csv(f, sep=\"\\t\", error_bad_lines=False)\n",
    "    \n",
    "df = df.dropna(subset=[\"commentsReview\"])\n",
    "documents = df['commentsReview'].apply(lambda x: preProc(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelmetzger/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:497: RuntimeWarning: overflow encountered in exp\n",
      "  expElogthetad = np.exp(Elogthetad)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in topic_term_dists sum to 1.\n * Not all rows (distributions) in doc_topic_dists sum to 1.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1531cffecc0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_bigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in topic_term_dists sum to 1.\n * Not all rows (distributions) in doc_topic_dists sum to 1."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "texts = [word_tokenize(doc)  for doc in documents]\n",
    "texts_lower = [[word.lower() for word in text] for text in texts]\n",
    "unigrams = [[word for word in text if not word.isdigit() and word not in stop_words and len(word) > 1] for text in texts_lower]\n",
    "\n",
    "bigrams = [[bigram for bigram in bigram_preprocess(text)] for text in unigrams]\n",
    "\n",
    "dictionary_bigrams = corpora.Dictionary(bigrams, prune_at=200000)\n",
    "dictionary_bigrams.save_as_text('gensim_dict_bigrams.txt')\n",
    "\n",
    "dictionary_unigrams = corpora.Dictionary(unigrams, prune_at=200000)\n",
    "dictionary_unigrams.save_as_text('gensim_dict_unigrams.txt')\n",
    "\n",
    "corpus = [dictionary_bigrams.doc2bow(text) for text in bigrams]\n",
    "lda = LdaModel(corpus, num_topics=20, id2word=dictionary_bigrams)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda, corpus, dictionary_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

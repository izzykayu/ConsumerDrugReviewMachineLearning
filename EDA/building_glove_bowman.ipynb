{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sec 2: Building GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run GloVe on the text of the Stanford Sentiment Treebank (SST) training set. Usually these methods are run on extremely large corpora, but we're using this here to make sure that you can train a reasonable model without waiting for hours or days. \n",
    "\n",
    "First, let's load the data as before. For our purposes, we won't need either the labels nor any of the test and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelmetzger/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>SHAPE</th>\n",
       "      <th>ALPHA</th>\n",
       "      <th>STOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TNF</td>\n",
       "      <td>tnf</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>XXX</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>punct</td>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alpha</td>\n",
       "      <td>alpha</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inhibitor</td>\n",
       "      <td>inhibitor</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pursued</td>\n",
       "      <td>pursue</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>treatment</td>\n",
       "      <td>treatment</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>psoriasis</td>\n",
       "      <td>psoriasis</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEXT      LEMMA    POS   TAG        DEP SHAPE  ALPHA   STOP\n",
       "0         TNF        tnf   NOUN    NN   compound   XXX   True  False\n",
       "1           -          -  PUNCT  HYPH      punct     -  False  False\n",
       "2       alpha      alpha   NOUN    NN   compound  xxxx   True  False\n",
       "3   inhibitor  inhibitor   NOUN    NN  nsubjpass  xxxx   True  False\n",
       "4          is         be   VERB   VBZ    auxpass    xx   True   True\n",
       "5     pursued     pursue   VERB   VBN       ROOT  xxxx   True  False\n",
       "6          in         in    ADP    IN       prep    xx   True   True\n",
       "7         the        the    DET    DT        det   xxx   True   True\n",
       "8   treatment  treatment   NOUN    NN       pobj  xxxx   True  False\n",
       "9          of         of    ADP    IN       prep    xx   True   True\n",
       "10  psoriasis  psoriasis   NOUN    NN       pobj  xxxx   True  False\n",
       "11          .          .  PUNCT     .      punct     .  False  False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('/Users/isabelmetzger/Downloads/LinguisticFeatures_csv.txt', delimiter='\\s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '/Users/isabelmetzger/MAE/dosingGuidelines.json/Annotation_of_CPIC_Guideline_for_abacavir_and_HLA_B.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annotation of CPIC Guideline for abacavir and HLA-B'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ujson\n",
    "with open(f) as fn:\n",
    "    json1 = ujson.load(fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_home = '../data/trees'\n",
    "\n",
    "import re\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed, we're only using the 250 most common words. Extract cooccurence counts from the corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "context_window = 4\n",
    "top_k = 250\n",
    "\n",
    "def tokenize(string):\n",
    "    string = string.lower()\n",
    "    return string.split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in training_set:\n",
    "    word_counter.update(tokenize(example['text']))\n",
    "vocabulary = [pair[0] for pair in word_counter.most_common()[0:top_k]]\n",
    "index_to_word_map = dict(enumerate(vocabulary))\n",
    "word_to_index_map = dict([(index_to_word_map[index], index) for index in index_to_word_map])\n",
    "\n",
    "def extract_cooccurrences(dataset, word_map, amount_of_context=context_window):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    for example in dataset:\n",
    "        words = tokenize(example['text'])\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_index_map:\n",
    "                continue\n",
    "            target_word_index = word_to_index_map[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context + 1)\n",
    "            for context_index in list(range(min_context_index, target_index)) + \\\n",
    "            list(range(target_index + 1, max_word)):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_index_map:\n",
    "                    continue\n",
    "                context_word_index = word_to_index_map[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(training_set, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def  batch_iter(nonzero_pairs, cooccurrences, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = len(nonzero_pairs)\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        word_i = []\n",
    "        word_j = []\n",
    "        counts = []\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [nonzero_pairs[index] for index in batch_indices]\n",
    "        for k in batch:\n",
    "            counts.append(cooccurrences[k])\n",
    "            word_i.append(k[0])\n",
    "            word_j.append(k[1])\n",
    "        yield [counts, word_i, word_j]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evalation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, a GloVe model trained on such a small dataset and vocabulary won't be spectacular, so we won't bother with a full-fledged similarity or analogy evaluation. Instead, we'll use the simple scoring function below, which grades the model on how well it captures ten easy/simple similarity comparisons. The function returns a score between 0 and 10. Random embeddings can be expected to get a score of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(model, word_one, word_two):\n",
    "    vec_one = model.get_embeddings(word_to_index_map[word_one]).reshape(1, -1)\n",
    "    vec_two = model.get_embeddings(word_to_index_map[word_two]).reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def score(model):\n",
    "    m = model\n",
    "    score = 0\n",
    "    score += similarity(m, 'a', 'an') > similarity(m, 'a', 'documentary')\n",
    "    score += similarity(m, 'in', 'of') > similarity(m, 'in', 'picture')\n",
    "    score += similarity(m, 'action', 'thriller') >  similarity(m, 'action', 'end')\n",
    "    score += similarity(m, 'films', 'movies') > similarity(m, 'films', 'good')\n",
    "    score += similarity(m, 'film', 'movie') > similarity(m, 'film', 'movies')\n",
    "    score += similarity(m, 'script', 'plot') > similarity(m, 'script', 'dialogue')\n",
    "    score += similarity(m, 'character', 'human') > similarity(m, 'character', 'young')\n",
    "    score += similarity(m, '``', \"''\") > similarity(m, '``', 'quite')\n",
    "    score += similarity(m, 'funny', 'entertaining') > similarity(m, 'funny', 'while')\n",
    "    score += similarity(m, 'good', 'great') > similarity(m, 'good', 'minutes')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've built and trained the model, you can evaluate it by calling `score(model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implement and train\n",
    "\n",
    "There's some starter code below for training a PyTorch model. **Fill it out to create an implementation of GloVe, then train it on the SST training set.**\n",
    "Try not to modify any of the starter code.\n",
    "\n",
    "### 2.3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, batch_size):\n",
    "        super(Glove, self).__init__()        \n",
    "        self.word_embeddings = None\n",
    "        \n",
    "        \"\"\"\n",
    "        Your code goes here.\n",
    "        \"\"\"\n",
    "            \n",
    "    \n",
    "    def forward():\n",
    "        \"\"\"\n",
    "        And here.\n",
    "        \"\"\"       \n",
    "        \n",
    "        return \n",
    "        \n",
    "        \n",
    "    def init_weights():\n",
    "        \"\"\"\n",
    "        And here.\n",
    "        \"\"\"\n",
    "    \n",
    "    def add_embeddings():\n",
    "        \"\"\"\n",
    "        And here.\n",
    "        \n",
    "        Give W_emb = W + W^tilde\n",
    "        \"\"\"\n",
    "        return self.word_embeddings\n",
    "    \n",
    "    def get_embeddings(self, index):\n",
    "        if self.word_embeddings is None:\n",
    "            add_embeddings()\n",
    "        return self.word_embeddings[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, optim, data_iter, xmax, alpha):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        counts, words, co_words = next(data_iter)        \n",
    "        words_var = Variable(torch.LongTensor(words))\n",
    "        co_words_var = Variable(torch.LongTensor(co_words))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        \"\"\"\n",
    "        Your code goes here.\n",
    "        \"\"\"\n",
    "        \n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                word_emebddings = model.add_embeddings()\n",
    "                print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch), \"Score:\", score(model) )\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Train a working model\n",
    "\n",
    "\n",
    "You should use the following commands to train the model for at least 2000 steps. If your model works, it will converge to a score of 10 within that many steps. You need to complete the previous code block before you can run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 1024\n",
    "learning_rate = 1.0\n",
    "num_epochs = 2000\n",
    "alpha = 0.75\n",
    "xmax = 50\n",
    "\n",
    "glove = Glove(embedding_dim, vocab_size, batch_size)\n",
    "glove.init_weights()\n",
    "optimizer = torch.optim.Adadelta(glove.parameters(), lr=learning_rate)\n",
    "data_iter = batch_iter(nonzero_pairs, cooccurrences, batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, glove, optimizer, data_iter, xmax, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
